{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the best submission of the team \"My little Poly\".\n",
    "The code runtime is quite long since many calculations are\n",
    "performed in the code. To predict one single set of lambda\n",
    "and gamma hyperparameters.\n",
    "NOTE: The plots where made on with Jupiter Notebook in order\n",
    "to access to the plot library\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from implementations import *\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 images\n",
      "Loading 100 images\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "import os\n",
    "# Load the training set\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = np.asarray([load_image(gt_dir + files[i]) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(GT_images, Satellite_images):\n",
    "    \n",
    "        group_size = 150\n",
    "        rect_size = 72 # This is the size of the window\n",
    "        patch_size=16  # This is a size of one side of the rectangle we use to analyse image.\n",
    "        \n",
    "        print('Training set shape: ', Satellite_images.shape)\n",
    "        \n",
    "        def generate_minibatch(): # Minibatch creation technics.\n",
    "        \n",
    "                # Generate one minibatch\n",
    "                Satellite_images_group = np.zeros((group_size, rect_size, rect_size, 3)) #j'aurais aussi pu utiliser np.empty\n",
    "                GT_images_group = np.emptyzeros((group_size, 2))\n",
    "                for i in range(group_size):\n",
    "                    # Select a random image\n",
    "                    number_image=Satellite_images.shape[0]\n",
    "                    index = np.random.choice(number_image) #https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html\n",
    "                    \n",
    "                    image_selected=Satellite_images[index]\n",
    "                    image_ground_selected = GT_images[index]\n",
    "                    image_selected_shape = Satellite_images[index].shape\n",
    "                    image_ground_selected_shape = GT_images[index].shape\n",
    "                    \n",
    "                    # Sample a random window from the image\n",
    "                    center = np.random.randint(rect_size//2, image_selected_shape[0] - rect_size//2, 2) #Return random integers from the “discrete uniform” distribution of the specified dtype in the “half-open” interval [low, high). #https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.randint.html\n",
    "                    sub_image = image_selected[center[0]-rect_size//2:center[0]+size//2,center[1]-rect_size//2:center[1]+rect_size//2]\n",
    "                    # We selected a part of the selected image next to the center that we choose randomly.\n",
    "                    image_window = image_ground_selected[center[0]-patch_size//2:center[0]+patch_size//2,center[1]-patch_size//2:center[1]+patch_size//2]\n",
    "                    # We selected the same part of the image_selected with the random center but this time we take it in the groundthruth image.\n",
    "                    \n",
    "                    # The label : if the mean of the pixel of the group is superior to the theresold, we thus consider that this is road and the label is one.\n",
    "                    mean = np.array([np.mean(image_window)])\n",
    "                    threshold = 0.35\n",
    "                    label = (mean > threshold) * 1\n",
    "                    # This is the true label that we obtain thanks to the groundthruth image\n",
    "    \n",
    "                    \n",
    "                    # We could have rotate the images rondomly there but we decide to do another way : we create new image wich are rotated\n",
    "\n",
    "                    label = np_utils.to_categorical(label, 2) #road =1 and not_a_road=0 --> Converts a class vector (integers) to binary class matrix.\n",
    "                    Satellite_images_group[i] = sub_image\n",
    "                    GT_images_group[i] = label\n",
    "                    \n",
    "                return (Satellite_images_group, GT_images_group)\n",
    "\n",
    " #dans dossier on a du code pour baisser le temps de computation et pour fair marcher keras et theanos ensemble\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape #number of images then number of line then number of column and then number of chanel(RBG because image is on color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400, 400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_imgs.shape #as theses images are in blck and white they have one less attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 400, 3)\n",
      "(40000, 400)\n"
     ]
    }
   ],
   "source": [
    "print (imgs.shape)\n",
    "colonne = len(imgs[0])\n",
    "nb_image = len(imgs)\n",
    "X=np.zeros((nb_image*colonne,colonne))\n",
    "for i in range (nb_image):\n",
    "    for j in range (colonne):\n",
    "        for k in range (colonne):\n",
    "            r=imgs[i,j,k,0]\n",
    "            g=imgs[i,j,k,1]\n",
    "            b=imgs[i,j,k,2]\n",
    "            rgb = 65536 * r + 256 * g + b;\n",
    "            X[j*i,k]=rgb\n",
    "print (X.shape)     \n",
    "#algo pas du tout optimal avec 48 000 000  calcul car je passe pixel par pixel   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 400)\n"
     ]
    }
   ],
   "source": [
    "colonne = len(gt_imgs[0])\n",
    "nb_image = len(gt_imgs)\n",
    "X_gt=np.zeros((nb_image*colonne,colonne))\n",
    "for i in range (nb_image):\n",
    "    for j in range (colonne):\n",
    "        for k in range (colonne):\n",
    "            X_gt[j*i,k]=gt_imgs[i,j,k]\n",
    "print (X_gt.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanning and Standardizing the trainning & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanning the missing values and standardizing the data\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleanning the missing values and standardizing the data\")\n",
    "# Removing bothering data and centering\n",
    "standardized_imgs, stddev_train_imgs, mean_train_imgs = standardize_original(X)\n",
    "standardized_gt_imgs, stddev_train_gt_imgs, mean_train_gt_imgs = standardize_original(X_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building polynomial basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building polynomial basis\n",
      "Creating indices for subsets of degree 2\n",
      "Creating indices for subsets of degree 3\n",
      "Computing first degree\n",
      "Computing second degree WITH combinations\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a4e92353c4e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Building polynomial basis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmat_train_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_basis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstandardized_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmat_train_gt_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_basis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstandardized_gt_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Standardizing the matrix contacting the Polynome\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\ML\\Projet2\\MyLittlePoly\\Logistic\\implementations.py\u001b[0m in \u001b[0;36mbuild_poly_basis\u001b[1;34m(tx)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Computing second degree WITH combinations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;31m# Second degree gotten from indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m     \u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstdX_Ncols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstdX_Ncols\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mindices_s_Ncols\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_s_deg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_s_deg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[1;31m#print(\"Computing from degree 3 to 10 WITHOUT combinations...\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Building polynomial basis\")\n",
    "mat_train_imgs = build_poly_basis(standardized_imgs)\n",
    "mat_train_gt_imgs = build_poly_basis(standardized_gt_imgs)\n",
    "\n",
    "print(\"Standardizing the matrix contacting the Polynome\")\n",
    "standardized_mat_imgs, stddev_poly_train_imgs, mean_poly_train_imgs = standardize_basis(mat_train_imgs)\n",
    "tx_imgs = np.c_[np.ones(standardized_mat_imgs.shape[0]), standardized_mat_imgs]\n",
    "\n",
    "standardized_mat_gt_imgs, stddev_poly_train_gt_imgs, mean_poly_train_gt_imgs = standardize_basis(mat_train_gt_imgs)\n",
    "tx_gt_imgs = np.c_[np.ones(standardized_mat_gt_imgs.shape[0]), standardized_mat_gt_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Training Begins Little Padawan Regression\")\n",
    "#print(\"Please wait while the training completes\")\n",
    "# Cross validating - FOR THE ENTIRE DATASET\n",
    "#x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = cross_validation(y, tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best found parameters for reg_logistic_regression where:\n",
    "# lambda = 3.74*10**-9\n",
    "# gamma  = 10**5\n",
    "\n",
    "#LAST TESTED VALUES\n",
    "#lambdas_ = np.linspace(10**-9, 4*10**-9, 4)\n",
    "#gammas_ = np.linspace(4.5*10**5, 5.5*10**5, 3)\n",
    "\n",
    "# Best found parameters for reg_logistic_regression\n",
    "lambdas_ = [3.74*10**-9]\n",
    "gammas_ = [10**5]\n",
    "max_iters = 25\n",
    "initial_w = np.zeros((len(x1[0])))\n",
    "logistic_weights = []\n",
    "loss = []\n",
    "losses = np.zeros((len(gammas_), len(lambdas_)))\n",
    "\n",
    "for i in range(len(lambdas_)):\n",
    "    for j in range(len(gammas_)):\n",
    "        print(lambdas_[i])\n",
    "        print(gammas_[j])\n",
    "        reg_logistic_w1,loss1 = reg_logistic_regression(y1,x1,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "        #reg_logistic_w2,loss2 = reg_logistic_regression(y2,x2,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "       # reg_logistic_w3,loss3 = reg_logistic_regression(y3,x3,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "       # reg_logistic_w4,loss4 = reg_logistic_regression(y4,x4,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "       # reg_logistic_w5,loss5 = reg_logistic_regression(y5,x5,lambdas_[i],initial_w,max_iters,gammas_[j])\n",
    "       # logistic_weights.append((reg_logistic_w1+reg_logistic_w2+reg_logistic_w3+reg_logistic_w4+reg_logistic_w5)/5)\n",
    "       # losses[i,j] = ((loss1+loss2+loss3+loss4+loss5)/5)\n",
    "       # loss.append((loss1+loss2+loss3+loss4+loss5)/5)\n",
    "\n",
    "#min_losses = np.amin(losses)\n",
    "#idx_min = np.argmin(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the testing data\")\n",
    "_, testx, ids_test = load_csv_data('test.csv')\n",
    "\n",
    "\n",
    "\n",
    "#if(PCA_flag):\n",
    " #   print(\"Remove dimensions according to PCA\")\n",
    "  #  standardized_testx = standardized_testx.dot(eig_vec)\n",
    "\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = 'test_set_images/test_'+str(i)+'/test_' + str(i) + '.png'\n",
    "    n = 50\n",
    "    print(\"Loading \" + str(n) + \" images\")\n",
    "    test = np.asarray([load_image(image_dir + files[i]) for i in range(n)])\n",
    "    \n",
    "print(\"Applying the same standardization to the testing set\")\n",
    "standardized_testx = standardize_test_original(test, mean_train_imgs, stddev_train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building polynomial basis\")\n",
    "mat_test = build_poly_basis(standardized_testx)\n",
    "\n",
    "print(\"Standardizing again\")\n",
    "standardized_testmat = standardized_testx_basis(mat_test, mean_poly_train, stddev_poly_train)\n",
    "tX_test = np.c_[np.ones(standardized_testmat.shape[0]), standardized_testmat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best hyperparamters Lambda and Gamma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic_weights_min =  logistic_weights[idx_min]\n",
    "\n",
    "#idx_lambdas = int(idx_min / len(lambdas_))\n",
    "#idx_gammas = idx_min % len(lambdas_)\n",
    "\n",
    "#print(\"The best submission parameter was [lambda]\", lambdas_[idx_lambdas])\n",
    "#print(\"The best submission parameter was [gamma]\", gammas_[idx_gammas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the final prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(reg_logistic_w1, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, 'predictions_ALL.csv')\n",
    "print(\"The prediction has been stored in the predictions.csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the prediction was accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control the accuracy of the output\n",
    "y_pred = predict_labels(logistic_weights_min, tx)\n",
    "counter = 0.0\n",
    "for i in range (0, len(y)):\n",
    "    if (y_pred[i] == y[i]):\n",
    "        counter = counter + 1\n",
    "print ('Accuracy', counter /len(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
